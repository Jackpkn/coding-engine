#!/usr/bin/env node

const http = require("http");
const { GroqLLMService, FallbackLLMService } = require("./out/llmService");

async function testFullLLM() {
  console.log("ğŸ¤– Testing Full LLM Integration with Real Codebase Context\n");

  async function makeRequest(method, endpoint, data = null) {
    return new Promise((resolve, reject) => {
      const url = new URL(endpoint, "http://localhost:3000");
      const options = {
        method,
        headers: { "Content-Type": "application/json" },
      };

      const req = http.request(url, options, (res) => {
        let body = "";
        res.on("data", (chunk) => (body += chunk));
        res.on("end", () => {
          try {
            resolve({ status: res.statusCode, data: JSON.parse(body) });
          } catch (e) {
            resolve({ status: res.statusCode, data: body });
          }
        });
      });

      req.on("error", reject);
      if (data) req.write(JSON.stringify(data));
      req.end();
    });
  }

  try {
    // 1. Test server connection
    console.log("1. ğŸ”Œ Testing Server Connection");
    const health = await makeRequest("GET", "/health");
    if (health.data.success) {
      console.log("   âœ… Server is running and healthy");
    } else {
      console.log("   âŒ Server health check failed");
      return;
    }

    // 2. Search for cpu_core to get real context
    console.log("\n2. ğŸ” Searching for cpu_core in codebase");
    const searchResult = await makeRequest("GET", "/search?query=cpu_core");

    if (!searchResult.data.success) {
      console.log("   âŒ Search failed:", searchResult.data.error);
      return;
    }

    const results = searchResult.data.data.results;
    console.log(`   âœ… Found ${results.length} results for cpu_core`);

    results.forEach((r, i) => {
      console.log(
        `      ${i + 1}. ${r.symbol?.name} (${r.symbol?.kind}) - ${r.file
          .split("/")
          .pop()}:${r.line + 1}`
      );
      console.log(`         Code: ${r.snippet}`);
    });

    // 3. Build context like the chat extension does
    console.log("\n3. ğŸ§  Building LLM Context");
    const context = {
      query: "what does cpu_core do?",
      searchResults: results,
      codeSnippets: results
        .filter((r) => r.snippet)
        .map((r) => `${r.file.split("/").pop()}:${r.line + 1} - ${r.snippet}`),
      modules: results
        .filter((r) => r.symbol?.kind === "module")
        .map((r) => r.symbol.name),
      signals: results
        .filter((r) => r.symbol?.kind === "signal" || r.symbol?.kind === "port")
        .map((r) => r.symbol.name),
    };

    console.log("   ğŸ“Š Context Summary:");
    console.log(`      Search Results: ${context.searchResults.length}`);
    console.log(`      Code Snippets: ${context.codeSnippets.length}`);
    console.log(`      Modules: ${context.modules.join(", ") || "None"}`);
    console.log(`      Signals: ${context.signals.join(", ") || "None"}`);

    // 4. Test LLM with real context
    console.log("\n4. ğŸ¤– Testing LLM with Real Context");

    const groqService = new GroqLLMService();
    const fallbackService = new FallbackLLMService();
    const llmService = groqService.isConfigured()
      ? groqService
      : fallbackService;

    console.log(
      `   Using: ${
        groqService.isConfigured() ? "Groq LLM" : "Fallback Service"
      }`
    );

    const llmResponse = await llmService.generateResponse(context);

    if (llmResponse.success) {
      console.log("   âœ… LLM Response Generated Successfully");
      console.log("\nğŸ“ LLM Response:");
      console.log("â”€".repeat(60));
      console.log(llmResponse.content);
      console.log("â”€".repeat(60));
    } else {
      console.log("   âŒ LLM Response Failed:", llmResponse.error);
    }

    // 5. Test different query types
    console.log("\n5. ğŸ”¬ Testing Different Query Types");

    const testQueries = [
      { query: "show me all modules", searchTerm: "module" },
      { query: "find the ALU", searchTerm: "alu" },
      { query: "what signals are in the system?", searchTerm: "logic" },
    ];

    for (const test of testQueries) {
      console.log(`\n   Testing: "${test.query}"`);

      const testSearch = await makeRequest(
        "GET",
        `/search?query=${test.searchTerm}`
      );
      if (testSearch.data.success) {
        const testResults = testSearch.data.data.results;
        console.log(`   ğŸ“Š Found ${testResults.length} results`);

        if (testResults.length > 0) {
          const testContext = {
            query: test.query,
            searchResults: testResults.slice(0, 3),
            codeSnippets: testResults
              .slice(0, 3)
              .filter((r) => r.snippet)
              .map(
                (r) => `${r.file.split("/").pop()}:${r.line + 1} - ${r.snippet}`
              ),
            modules: testResults
              .filter((r) => r.symbol?.kind === "module")
              .map((r) => r.symbol.name),
            signals: testResults
              .filter((r) => r.symbol?.kind === "signal")
              .map((r) => r.symbol.name),
          };

          const testLLMResponse = await llmService.generateResponse(
            testContext
          );
          if (testLLMResponse.success) {
            console.log(
              `   âœ… LLM Response: ${testLLMResponse.content.substring(
                0,
                100
              )}...`
            );
          } else {
            console.log(`   âŒ LLM Failed: ${testLLMResponse.error}`);
          }
        }
      }
    }

    console.log("\nğŸ‰ Full LLM Integration Test Complete!");
    console.log("\nğŸ“‹ Summary:");
    console.log("âœ… Server is running and responding");
    console.log("âœ… Search finds modules and symbols in codebase");
    console.log("âœ… Context is built correctly with real code snippets");
    console.log("âœ… LLM generates intelligent responses based on context");
    console.log("âœ… Different query types work correctly");

    console.log("\nğŸš€ Your VS Code chat extension should now provide:");
    console.log("   â€¢ General Verilog answers for syntax questions");
    console.log("   â€¢ Code-specific analysis using your actual modules");
    console.log(
      "   â€¢ Intelligent explanations with context from your codebase"
    );
  } catch (error) {
    console.error("âŒ Test failed:", error.message);
  }
}

testFullLLM();
